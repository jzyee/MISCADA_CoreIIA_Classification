---
title: "R Notebook"
output: html_notebook
---

this notebook will be specifically for inception v3


```{r}
test
```


```{r}
train_y <- read.csv('create_images/train/class_list.csv')[2]

test_y <- read.csv('create_images/test/class_list.csv')[2]



#val_y <- read.csv('val/class_list.csv')[2]

```

```{r}
change_to_binary_problem <- function(class_list){
  class_list[class_list != 1] <- 0

  for(x in 0:5){
    print(length(class_list[class_list == x]))
  }
  return(class_list)
}

train_y <- change_to_binary_problem(train_y)
test_y <- change_to_binary_problem(test_y)
```


```{r}
library(magick)
library(png)
library(imager)
# <- array(rep(1, 1*218*230*3 ), dim=c(1,218,230,3))
sample_1 <- imager::load.image('train/img1000_y4.png')
image_bu <- array(rep(sample_1), dim=c(1,218,230,3))
str(image_bu)
# str(image_data(sample_1,'rgb'))#reads in as a greyscale image
# sample_1 <- image_scale(sample_1, '200')
# sample_1 <- image_data(sample_1,'rgb')
# str(sample_1)

```


```{r}
library("rsample")
set.seed(212)

library(reticulate)
use_python('C:/Users/Anaconda3')

load_img_data <- function(folder_name){
  
  files <- list.files(path=folder_name, pattern="*.png", full.names=TRUE, recursive=FALSE)
  print(length(files))
  output <- array(rep(0, length(files)*150*150*3 ), dim=c(length(files),150,150,3))
  print(str(output))
  for(idx in 1:length(files)){
    # x <- image_read(files[idx])
    # x <- image_scale(x, '200')
    # x <- image_data(x,'rgb')
    x <- imager::load.image(files[idx])
    x <- imager::resize(x, 150, 150)
    x <- array(rep(x), dim=c(1,150,150,3))
    x <- array_reshape(x, c(1,150,150,3))
    output[idx,,,] <- x
  }
  # output <- imagenet_preprocess_input(output)
  return(output)
}
#memory.limit(size=4000)
train_x <- load_img_data('create_images/train')
print(str(train_x))
#train_x <- imager::load.image('train/img1_y4.jpg')

```

```{r}
test_x <- load_img_data('create_images/test')
str(test_x)
```
```{r}
str(train_y)
```


#need shuffling
```{r}
set.seed(101)
rows_idx <- sample(nrow(train_y))
train_y <- train_y[rows_idx,]
train_x <- train_x[rows_idx,,,]
rows_idx2 <-sample(nrow(test_y))
test_y <- test_y[rows_idx2,]
test_x <- test_x[rows_idx2,,,]
```



```{r}
# not needed for model making


#max(train_x)
#min(train_x)
plot(unlist(epil_train_final[1,]),type='l')
plot(as.raster(train_x[1,,,]))
print(train_y[1,])
hist(train_x[1,,,])
```

```{r}
test_y[1:10]
```


```{r}
library(reticulate)
use_python('C:/Users/Anaconda3/condabin') 
library(keras)
use_session_with_seed(44)

train_y.cat <- to_categorical(train_y)[,-1]
head(train_y.cat)

#train_y
```

```{r}
test_y.cat <- to_categorical(test_y)[,-1]
head(test_y.cat)
```


```{r}
basic_v3 <- application_inception_v3(weights = 'imagenet',
                                         include_top = FALSE,
                                         input_shape = c(150,150,3))
```



```{r}
predictions <- basic_v3$output %>%
  layer_global_average_pooling_2d() %>%
  layer_dense(units=1024, activation='relu') %>%
  layer_dropout(rate=0.5) %>%
  layer_dense(units=512, activation='relu') %>%
  layer_dropout(rate=0.5) %>%
  layer_dense(units=512, activation='relu') %>%
  layer_dropout(rate=0.5) %>%
  layer_dense(units=1, activation='sigmoid')

stacked_v3 <- keras_model(input = basic_v3$input, output = predictions)

freeze_weights(basic_v3)

```



```{r}
stacked_v3 %>% compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics='accuracy')
```


```{r}
history <- stacked_v3 %>% fit(train_x,
  train_y.cat,
  epochs = 10,
  batch_size = 50,
  validation_split = 0.2
)
```





```{r}
stacked_v3 %>% evaluate(test_x, test_y.cat)

```


```{r}
layers <- basic_v3$layers
for (i in 1:length(layers))
     cat(i, layers[[i]]$name, '\n')
```

```{r}
#train top 2 inception blocks 
freeze_weights(basic_v3, from=1, to=172)
unfreeze_weights(basic_v3, from=173)
```


```{r}
stacked_v3 %>% compile(
  optimizer=optimizer_sgd(lr=0.0001),
  loss='binary_crossentropy',
  metrics='accuracy'
)
```


```{r}
history <- stacked_v3 %>% fit(train_x,
  train_y.cat,
  epochs = 10,
  batch_size = 50,
  validation_split = 0.2
)

```

```{r}
stacked_v3 %>% evaluate(test_x, test_y.cat)
```


